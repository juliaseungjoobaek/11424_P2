# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FRRuQ0ddK9mE6-VNOst9evxG1Si416uA
"""

from google.colab import drive
drive.mount('/content/drive')

import unicodedata

def evaluate(gold, pred):

  preds = [int(unicodedata.normalize("NFC",p)==unicodedata.normalize("NFC",g)) for p, g in zip(pred, gold)]
  if len(preds) == 0:
    return 0
  return sum(preds)/len(preds)

import sys, os, getopt, re
from functools import wraps
from glob import glob

def hamming(s,t):
    return sum(1 for x,y in zip(s,t) if x != y)


def halign(s,t):
    """Align two strings by Hamming distance."""
    slen = len(s)
    tlen = len(t)
    minscore = len(s) + len(t) + 1
    for upad in range(0, len(t)+1):
        upper = '_' * upad + s + (len(t) - upad) * '_'
        lower = len(s) * '_' + t
        score = hamming(upper, lower)
        if score < minscore:
            bu = upper
            bl = lower
            minscore = score

    for lpad in range(0, len(s)+1):
        upper = len(t) * '_' + s
        lower = (len(s) - lpad) * '_' + t + '_' * lpad
        score = hamming(upper, lower)
        if score < minscore:
            bu = upper
            bl = lower
            minscore = score

    zipped = zip(bu,bl)
    newin  = ''.join(i for i,o in zipped if i != '_' or o != '_')
    newout = ''.join(o for i,o in zipped if i != '_' or o != '_')
    return newin, newout


def levenshtein(s, t, inscost = 1.0, delcost = 1.0, substcost = 1.0):
    """Recursive implementation of Levenshtein, with alignments returned."""
    @memolrec
    def lrec(spast, tpast, srem, trem, cost):
        if len(srem) == 0:
            return spast + len(trem) * '_', tpast + trem, '', '', cost + len(trem)
        if len(trem) == 0:
            return spast + srem, tpast + len(srem) * '_', '', '', cost + len(srem)

        addcost = 0
        if srem[0] != trem[0]:
            addcost = substcost

        return min((lrec(spast + srem[0], tpast + trem[0], srem[1:], trem[1:], cost + addcost),
                   lrec(spast + '_', tpast + trem[0], srem, trem[1:], cost + inscost),
                   lrec(spast + srem[0], tpast + '_', srem[1:], trem, cost + delcost)),
                   key = lambda x: x[4])

    answer = lrec('', '', s, t, 0)
    return answer[0],answer[1],answer[4]


def memolrec(func):
    """Memoizer for Levenshtein."""
    cache = {}
    @wraps(func)
    def wrap(sp, tp, sr, tr, cost):
        if (sr,tr) not in cache:
            res = func(sp, tp, sr, tr, cost)
            cache[(sr,tr)] = (res[0][len(sp):], res[1][len(tp):], res[4] - cost)
        return sp + cache[(sr,tr)][0], tp + cache[(sr,tr)][1], '', '', cost + cache[(sr,tr)][2]
    return wrap


def alignprs(lemma, form):
    """Break lemma/form into three parts:
    IN:  1 | 2 | 3
    OUT: 4 | 5 | 6
    1/4 are assumed to be prefixes, 2/5 the stem, and 3/6 a suffix.
    1/4 and 3/6 may be empty.
    """

    al = levenshtein(lemma, form, substcost = 1.1) # Force preference of 0:x or x:0 by 1.1 cost
    alemma, aform = al[0], al[1]
    # leading spaces
    lspace = max(len(alemma) - len(alemma.lstrip('_')), len(aform) - len(aform.lstrip('_')))
    # trailing spaces
    tspace = max(len(alemma[::-1]) - len(alemma[::-1].lstrip('_')), len(aform[::-1]) - len(aform[::-1].lstrip('_')))
    return alemma[0:lspace], alemma[lspace:len(alemma)-tspace], alemma[len(alemma)-tspace:], aform[0:lspace], aform[lspace:len(alemma)-tspace], aform[len(alemma)-tspace:]


def prefix_suffix_rules_get(lemma, form):
    """Extract a number of suffix-change and prefix-change rules
    based on a given example lemma+inflected form."""
    lp,lr,ls,fp,fr,fs = alignprs(lemma, form) # Get six parts, three for in three for out

    # Suffix rules
    ins  = lr + ls + ">"
    outs = fr + fs + ">"
    srules = set()
    for i in range(min(len(ins), len(outs))):
        srules.add((ins[i:], outs[i:]))
    srules = {(x[0].replace('_',''), x[1].replace('_','')) for x in srules}

    # Prefix rules
    prules = set()
    if len(lp) >= 0 or len(fp) >= 0:
        inp = "<" + lp
        outp = "<" + fp
        for i in range(0,len(fr)):
            prules.add((inp + fr[:i],outp + fr[:i]))
            prules = {(x[0].replace('_',''), x[1].replace('_','')) for x in prules}

    return prules, srules


def apply_best_rule(lemma, msd, allprules, allsrules):
    """Applies the longest-matching suffix-changing rule given an input
    form and the MSD. Length ties in suffix rules are broken by frequency.
    For prefix-changing rules, only the most frequent rule is chosen."""

    bestrulelen = 0
    base = "<" + lemma + ">"
    if msd not in allprules and msd not in allsrules:
        return lemma # Haven't seen this inflection, so bail out

    if msd in allsrules:
        applicablerules = [(x[0],x[1],y) for x,y in allsrules[msd].items() if x[0] in base]
        if applicablerules:
            bestrule = max(applicablerules, key = lambda x: (len(x[0]), x[2], len(x[1])))
            base = base.replace(bestrule[0], bestrule[1])

    if msd in allprules:
        applicablerules = [(x[0],x[1],y) for x,y in allprules[msd].items() if x[0] in base]
        if applicablerules:
            bestrule = max(applicablerules, key = lambda x: (x[2]))
            base = base.replace(bestrule[0], bestrule[1])

    base = base.replace('<', '')
    base = base.replace('>', '')
    return base


def numleadingsyms(s, symbol):
    return len(s) - len(s.lstrip(symbol))


def numtrailingsyms(s, symbol):
    return len(s) - len(s.rstrip(symbol))

def apply_chained_rules(lemma, msd):
    """Applies prefix + suffix rules in sequence."""
    step1 = suffix_fst.apply(lemma)
    step2 = prefix_fst.apply(step1)

    # If the output still matches the lemma, no rule worked
    if step2 == lemma:
        return lemma  # Return unchanged lemma

    return step2  # Return transformed word

'''def hybrid_predict(lemma, msd):
    # Try rule-based transformation first
    rule_output = apply_best_rule(lemma, msd, allprules, allsrules)

    # If rule-based model fails (returns unchanged lemma), use neural model
    if rule_output == lemma:
        return neural_model.predict(lemma, msd)  # Call ML model

    return rule_output  # Return rule-based output if successful
'''

from torch.nn.utils.rnn import pad_sequence

class InflectionDataset(Dataset):
    def __init__(self, file_path):
        # Load dataset
        with open(file_path, "r", encoding="utf-8") as f:
            lines = [line.strip().split("\t") for line in f.readlines()]

        # Extract lemma-inflected pairs
        self.data = [(lemma, form) for lemma, form, msd in lines]

        # Build vocabulary
        self.vocab = list(set("".join([w for pair in self.data for w in pair]))) + ["<EOS>", "<PAD>"]
        self.char_to_idx = {char: idx for idx, char in enumerate(self.vocab)}
        self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        lemma, inflected = self.data[idx]

        # Convert characters to indices
        lemma_seq = [self.char_to_idx[c] for c in lemma] + [self.char_to_idx["<EOS>"]]
        inflected_seq = [self.char_to_idx[c] for c in inflected] + [self.char_to_idx["<EOS>"]]

        return torch.tensor(lemma_seq), torch.tensor(inflected_seq)

# Load dataset
lang = 'xty'
dataset_path = f"/content/drive/MyDrive/Project2/dataset/{lang}.train.tsv"
dataset = InflectionDataset(dataset_path)

from torch.nn.utils.rnn import pad_sequence

def collate_fn(batch):
    lemmas, inflecteds = zip(*batch)

    # Get the maximum sequence length in the batch
    max_len = max(max(len(l) for l in lemmas), max(len(i) for i in inflecteds))

    # Pad sequences to the max length in the batch
    lemmas_padded = [torch.cat([l, torch.full((max_len - len(l),), dataset.char_to_idx["<PAD>"])]) for l in lemmas]
    inflecteds_padded = [torch.cat([i, torch.full((max_len - len(i),), dataset.char_to_idx["<PAD>"])]) for i in inflecteds]

    return torch.stack(lemmas_padded), torch.stack(inflecteds_padded)

# Create DataLoader with fixed padding
dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)

import torch.nn as nn
import torch.optim as optim

class TransformerInflection(nn.Module):
    def __init__(self, vocab_size, emb_dim=128, num_heads=4, hidden_dim=256):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, emb_dim)
        self.encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=emb_dim, nhead=num_heads, dim_feedforward=hidden_dim),
            num_layers=3
        )
        self.decoder = nn.Linear(emb_dim, vocab_size)

    def forward(self, x):
        x = self.embedding(x)
        x = self.encoder(x)
        x = self.decoder(x)
        return x

# Define model
model = TransformerInflection(vocab_size=len(dataset.vocab))
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss(ignore_index=dataset.char_to_idx["<PAD>"])  # Ignore padding

# Training Loop
num_epochs = 20
PAD_INDEX = dataset.char_to_idx["<PAD>"]  # Get padding index

for epoch in range(num_epochs):
    total_loss = 0
    for lemma, inflected in dataloader:
        optimizer.zero_grad()

        # Forward pass
        output = model(lemma)  # Output shape: [batch_size, seq_length, vocab_size]

        # Ensure correct tensor dimensions
        batch_size, seq_length, vocab_size = output.shape

        # Reshape tensors for CrossEntropyLoss
        output = output.reshape(batch_size * seq_length, vocab_size)  # [total_tokens, vocab_size]
        inflected = inflected.reshape(batch_size * seq_length)  # [total_tokens]

        # Compute loss (ignore padding tokens)
        loss = criterion(output, inflected)

        # Backward pass
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader)}")

def hybrid_predict(lemma, msd):
    rule_output = apply_best_rule(lemma, msd, allprules, allsrules)

    # If rule-based model fails (unchanged lemma), switch to Transformer
    if rule_output == lemma:
        transformer_input = torch.tensor([dataset.char_to_idx[c] for c in lemma] + [dataset.char_to_idx["<EOS>"]]).unsqueeze(0)
        output = model(transformer_input)
        output_seq = [dataset.idx_to_char[idx] for idx in output.argmax(dim=-1).squeeze().tolist()]
        return "".join(output_seq).replace("<EOS>", "")

    return rule_output  # Rule-based output if successful

import torch

lang = 'xty'

allprules, allsrules = {}, {}
lines = [line.strip() for line in open(f"/content/drive/MyDrive/Project2/dataset/{lang}.train.tsv", "r") if line != '\n']
trainlemmas = set()
trainmsds = set()

# Detect prefixing or suffixing tendency
prefbias, suffbias = 0, 0
for l in lines:
    lemma, form, msd = l.split(u'\t')
    trainlemmas.add(lemma)
    trainmsds.add(msd)
    aligned = halign(lemma, form)
    if ' ' not in aligned[0] and ' ' not in aligned[1] and '-' not in aligned[0] and '-' not in aligned[1]:
        prefbias += numleadingsyms(aligned[0], '_') + numleadingsyms(aligned[1], '_')
        suffbias += numtrailingsyms(aligned[0], '_') + numtrailingsyms(aligned[1], '_')

# Extract transformation rules
for l in lines:
    lemma, form, msd = l.split(u'\t')
    if prefbias > suffbias:
        lemma = lemma[::-1]
        form = form[::-1]
    prules, srules = prefix_suffix_rules_get(lemma, form)

    if msd not in allprules and len(prules) > 0:
        allprules[msd] = {}
    if msd not in allsrules and len(srules) > 0:
        allsrules[msd] = {}

    for r in prules:
        allprules[msd][(r[0], r[1])] = allprules[msd].get((r[0], r[1]), 0) + 1

    for r in srules:
        allsrules[msd][(r[0], r[1])] = allsrules[msd].get((r[0], r[1]), 0) + 1

# Evaluation phase with hybrid prediction
evallines = [line.strip() for line in open(f"/content/drive/MyDrive/Project2/dataset/{lang}.dev.tsv", "r") if line != '\n']
outfile = open(f"{lang}.txt", "w")

pred = []
gold = []

def hybrid_predict(lemma, msd):
    rule_output = apply_best_rule(lemma, msd, allprules, allsrules)

    # If rule-based model fails (unchanged lemma), switch to Transformer
    if rule_output == lemma:
        transformer_input = torch.tensor([dataset.char_to_idx.get(c, dataset.char_to_idx["<PAD>"]) for c in lemma] + [dataset.char_to_idx["<EOS>"]]).unsqueeze(0)

        with torch.no_grad():  # Disable gradient tracking for inference
            output = model(transformer_input)

        output_seq = [dataset.idx_to_char[idx] for idx in output.argmax(dim=-1).squeeze().tolist()]
        return "".join(output_seq).replace("<EOS>", "")

    return rule_output  # Rule-based output if successful
'''
for l in evallines:
    lemma, correct, msd = l.split(u'\t')
    if prefbias > suffbias:
        lemma = lemma[::-1]

    outform = hybrid_predict(lemma, msd)

    if prefbias > suffbias:
        outform = outform[::-1]
        lemma = lemma[::-1]

    pred.append(outform)
    gold.append(correct)

    outfile.write(outform + "\n")
'''
print(pred[:10])
print(gold[:10])
print(evaluate(pred, gold))

# Load the test dataset (only contains lemma and MSD)
test_file = f"/content/drive/MyDrive/Project2/dataset/{lang}.test.tsv"
output_file = f"/content/{lang}.txt"

testlines = [line.strip() for line in open(test_file, "r") if line != '\n']

# Open output file to write predictions
with open(output_file, "w", encoding="utf-8") as outfile:
    for l in testlines:
        lemma, msd = l.split(u'\t')  # Extract lemma and MSD

        # Apply hybrid prediction
        if prefbias > suffbias:
            lemma = lemma[::-1]  # Reverse if the language is prefixing

        predicted_form = hybrid_predict(lemma, msd)

        if prefbias > suffbias:
            predicted_form = predicted_form[::-1]  # Reverse back if needed

        # Write to file
        outfile.write(predicted_form + "\n")

print(f"Predictions saved to: {output_file}")

"""# Original"""

lang = 'xty'

allprules, allsrules = {}, {}
lines = [line.strip() for line in open(f"/content/drive/MyDrive/Project2/dataset/{lang}.train.tsv", "r") if line != '\n']
trainlemmas = set()
trainmsds = set()

# First, test if language is predominantly suffixing or prefixing
# If prefixing, work with reversed strings
prefbias, suffbias = 0,0
for l in lines:
  lemma, form, msd = l.split(u'\t')
  trainlemmas.add(lemma)
  trainmsds.add(msd)
  aligned = halign(lemma, form)
  if ' ' not in aligned[0] and ' ' not in aligned[1] and '-' not in aligned[0] and '-' not in aligned[1]:
      prefbias += numleadingsyms(aligned[0],'_') + numleadingsyms(aligned[1],'_')
      suffbias += numtrailingsyms(aligned[0],'_') + numtrailingsyms(aligned[1],'_')
for l in lines: # Read in lines and extract transformation rules from pairs
    lemma, form, msd = l.split(u'\t')
    if prefbias > suffbias:
        lemma = lemma[::-1]
        form = form[::-1]
    prules, srules = prefix_suffix_rules_get(lemma, form)

    if msd not in allprules and len(prules) > 0:
        allprules[msd] = {}
    if msd not in allsrules and len(srules) > 0:
        allsrules[msd] = {}

    for r in prules:
        if (r[0],r[1]) in allprules[msd]:
            allprules[msd][(r[0],r[1])] = allprules[msd][(r[0],r[1])] + 1
        else:
            allprules[msd][(r[0],r[1])] = 1

    for r in srules:
        if (r[0],r[1]) in allsrules[msd]:
            allsrules[msd][(r[0],r[1])] = allsrules[msd][(r[0],r[1])] + 1
        else:
            allsrules[msd][(r[0],r[1])] = 1

evallines = [line.strip() for line in open(f"/content/drive/MyDrive/Project2/dataset/{lang}.dev.tsv", "r") if line != '\n']
outfile = open(f"{lang}.txt", "w")

pred = []
gold = []

for l in evallines:
    lemma, correct, msd = l.split(u'\t')
    if prefbias > suffbias:
        lemma = lemma[::-1]
    outform = apply_best_rule(lemma, msd, allprules, allsrules)
    if prefbias > suffbias:
        outform = outform[::-1]
        lemma = lemma[::-1]
    pred.append(outform)
    gold.append(correct)

    outfile.write(outform + "\n")

print(pred[:10])
print(gold[:10])
print(evaluate(pred, gold))

!git clone https://github.com/shijie-wu/neural-transducer/

!pip install --upgrade torch==1.13.1

# Commented out IPython magic to ensure Python compatibility.
run_train = """
#!/bin/bash
lang=$1
arch=${2:-tagtransformer}
suff=$3

lr=0.001
scheduler=warmupinvsqr
epochs=10
warmup=100
beta2=0.98       # 0.999
label_smooth=0.1 # 0.0
total_eval=50
bs=400 # 256

# transformer
layers=4
hs=1024
embed_dim=256
nb_heads=4
#dropout=${2:-0.3}
dropout=0.3
ckpt_dir=checkpoints/sig22

path=../dataset

python src/train.py \
    --dataset sigmorphon17task1 \
    --train $path/$lang.train.tsv \
    --dev $path/$lang.dev.tsv \
    --test $path/$lang.testhidden.tsv \
    --model $ckpt_dir/$arch/$lang \
    --decode greedy --max_decode_len 32 \
    --embed_dim $embed_dim --src_hs $hs --trg_hs $hs --dropout $dropout --nb_heads $nb_heads \
    --label_smooth $label_smooth --total_eval $total_eval \
    --src_layer $layers --trg_layer $layers --max_norm 1 --lr $lr --shuffle \
    --arch $arch --gpuid 0 --estop 1e-8 --bs $bs --epochs $epochs \
    --scheduler $scheduler --warmup_steps $warmup --cleanup_anyway --beta2 $beta2 --bestacc
"""

# %cd neural-transducer/
with open('run_train.sh', 'w') as f:
  f.write(run_train)
!make
!bash run_train.sh xty

# This function adds a dummy second column to your test set so that you can use the below decoding script to get predictions!
# https://github.com/shijie-wu/neural-transducer/blob/master/src/sigmorphon19-task1-decode.py

import csv

lang = 'xty'

def insert_column(input_tsv, output_tsv):
    with open(input_tsv, 'r', newline='', encoding='utf-8') as infile, \
         open(output_tsv, 'w', newline='', encoding='utf-8') as outfile:

        reader = csv.reader(infile, delimiter='\t')
        writer = csv.writer(outfile, delimiter='\t')

        for row in reader:
            if len(row) > 1:  # Ensure there are at least two columns
                new_row = [row[0], '#'] + row[1:]
            else:
                new_row = row + ['#']  # Edge case where only one column exists
            writer.writerow(new_row)

# Usage
insert_column(f"/content/drive/MyDrive/Project2/dataset/{lang}.test.tsv", f"/content/dataset/{lang}.tsv")



"""claude

"""