# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TAnkyEW2XHl4qzs4fnKkZilBTNoMuM4u
"""

import unicodedata

def evaluate(gold, pred):

  preds = [int(unicodedata.normalize("NFC",p)==unicodedata.normalize("NFC",g)) for p, g in zip(pred, gold)]
  if len(preds) == 0:
    return 0
  print(sum(preds), len(preds))
  return sum(preds)/len(preds)

import sys, os, getopt, re
from functools import wraps
from glob import glob
def hamming(s,t):
    return sum(1 for x,y in zip(s,t) if x != y)


def halign(s,t):
    """Align two strings by Hamming distance."""
    slen = len(s)
    tlen = len(t)
    minscore = len(s) + len(t) + 1
    for upad in range(0, len(t)+1):
        upper = '_' * upad + s + (len(t) - upad) * '_'
        lower = len(s) * '_' + t
        score = hamming(upper, lower)
        if score < minscore:
            bu = upper
            bl = lower
            minscore = score

    for lpad in range(0, len(s)+1):
        upper = len(t) * '_' + s
        lower = (len(s) - lpad) * '_' + t + '_' * lpad
        score = hamming(upper, lower)
        if score < minscore:
            bu = upper
            bl = lower
            minscore = score

    zipped = zip(bu,bl)
    newin  = ''.join(i for i,o in zipped if i != '_' or o != '_')
    newout = ''.join(o for i,o in zipped if i != '_' or o != '_')
    return newin, newout


def levenshtein(s, t, inscost = 1.0, delcost = 1.0, substcost = 0.9):
    """Recursive implementation of Levenshtein, with alignments returned."""
    @memolrec
    def lrec(spast, tpast, srem, trem, cost):
        if len(srem) == 0:
            return spast + len(trem) * '_', tpast + trem, '', '', cost + len(trem)
        if len(trem) == 0:
            return spast + srem, tpast + len(srem) * '_', '', '', cost + len(srem)

        addcost = 0
        if srem[0] != trem[0]:
            addcost = substcost

        return min((lrec(spast + srem[0], tpast + trem[0], srem[1:], trem[1:], cost + addcost),
                   lrec(spast + '_', tpast + trem[0], srem, trem[1:], cost + inscost),
                   lrec(spast + srem[0], tpast + '_', srem[1:], trem, cost + delcost)),
                   key = lambda x: x[4])

    answer = lrec('', '', s, t, 0)
    return answer[0],answer[1],answer[4]


def memolrec(func):
    """Memoizer for Levenshtein."""
    cache = {}
    @wraps(func)
    def wrap(sp, tp, sr, tr, cost):
        if (sr,tr) not in cache:
            res = func(sp, tp, sr, tr, cost)
            cache[(sr,tr)] = (res[0][len(sp):], res[1][len(tp):], res[4] - cost)
        return sp + cache[(sr,tr)][0], tp + cache[(sr,tr)][1], '', '', cost + cache[(sr,tr)][2]
    return wrap


def alignprs(lemma, form):
    """Break lemma/form into three parts:
    IN:  1 | 2 | 3
    OUT: 4 | 5 | 6
    1/4 are assumed to be prefixes, 2/5 the stem, and 3/6 a suffix.
    1/4 and 3/6 may be empty.
    """

    al = levenshtein(lemma, form, substcost = 1.1) # Force preference of 0:x or x:0 by 1.1 cost
    alemma, aform = al[0], al[1]
    # leading spaces
    lspace = max(len(alemma) - len(alemma.lstrip('_')), len(aform) - len(aform.lstrip('_')))
    # trailing spaces
    tspace = max(len(alemma[::-1]) - len(alemma[::-1].lstrip('_')), len(aform[::-1]) - len(aform[::-1].lstrip('_')))
    return alemma[0:lspace], alemma[lspace:len(alemma)-tspace], alemma[len(alemma)-tspace:], aform[0:lspace], aform[lspace:len(alemma)-tspace], aform[len(alemma)-tspace:]


def prefix_suffix_rules_get(lemma, form):
    """Extract a number of suffix-change and prefix-change rules
    based on a given example lemma+inflected form."""
    lp,lr,ls,fp,fr,fs = alignprs(lemma, form) # Get six parts, three for in three for out

    # Suffix rules
    ins  = lr + ls + ">"
    outs = fr + fs + ">"
    srules = set()
    for i in range(min(len(ins), len(outs))):
        srules.add((ins[i:], outs[i:]))
    srules = {(x[0].replace('_',''), x[1].replace('_','')) for x in srules}

    # Prefix rules
    prules = set()
    if len(lp) >= 0 or len(fp) >= 0:
        inp = "<" + lp
        outp = "<" + fp
        for i in range(0,len(fr)):
            prules.add((inp + fr[:i],outp + fr[:i]))
            prules = {(x[0].replace('_',''), x[1].replace('_','')) for x in prules}

    return prules, srules


def apply_best_rule(lemma, msd, allprules, allsrules):
    """Applies the longest-matching suffix-changing rule given an input
    form and the MSD. Length ties in suffix rules are broken by frequency.
    For prefix-changing rules, only the most frequent rule is chosen."""

    bestrulelen = 0
    base = "<" + lemma + ">"
    if msd not in allprules and msd not in allsrules:
        return lemma # Haven't seen this inflection, so bail out

    if msd in allsrules:
        applicablerules = [(x[0],x[1],y) for x,y in allsrules[msd].items() if x[0] in base]
        if applicablerules:
            bestrule = max(applicablerules, key = lambda x: (len(x[0]), x[2], len(x[1])))
            base = base.replace(bestrule[0], bestrule[1])

    if msd in allprules:
        applicablerules = [(x[0],x[1],y) for x,y in allprules[msd].items() if x[0] in base]
        if applicablerules:
            bestrule = max(applicablerules, key = lambda x: (x[2]))
            base = base.replace(bestrule[0], bestrule[1])

    base = base.replace('<', '')
    base = base.replace('>', '')

    return base


def numleadingsyms(s, symbol):
    return len(s) - len(s.lstrip(symbol))


def numtrailingsyms(s, symbol):
    return len(s) - len(s.rstrip(symbol))

soft_mutations = {'—ç—Ö—ç': '—Ö—ç', '—É—ç—Ö—ç': '—É—Ö—ç'}

def apply_soft_mutations(word):
    for k, v in soft_mutations.items():
        word = word.replace(k, v)
    return word

import re

def apply_best_rule(lemma, msd, allprules, allsrules):
    """Minimal changes to original rule-based model for fixing minor suffix errors."""

    base = "<" + lemma + ">"
    if msd not in allprules and msd not in allsrules:
        return lemma  # Haven't seen this inflection, so return lemma unchanged

    # **1. Apply suffix rules first**
    if msd in allsrules:
        applicablerules = [(x[0], x[1], y) for x, y in allsrules[msd].items() if x[0] in base]
        if applicablerules:
            bestrule = max(applicablerules, key=lambda x: (len(x[0]), x[2], len(x[1])))
            base = base.replace(bestrule[0], bestrule[1])

    # **2. Apply prefix rules (if applicable)**
    if msd in allprules:
        applicablerules = [(x[0], x[1], y) for x, y in allprules[msd].items() if x[0] in base]
        if applicablerules:
            bestrule = max(applicablerules, key=lambda x: x[2])
            base = base.replace(bestrule[0], bestrule[1])

    base = base.replace("<", "").replace(">", "")

    # **3. Controlled Fix: Ensure missing "—ç—Ö" before suffixes**
    if any(tag in msd for tag in ["PL", "INS", "ERG"]) and "—ç—Ö" in lemma:
        base = base.rstrip("—Ä") + "—ç—Ö"  # More general fix, allows soft variations

    # **4. Prevent over-inserting "—ç—ç" where it shouldn't be**
    base = re.sub(r'(?<=.)—ç—ç(?=[–∞-—è])', '—ç', base)  # Keeps initial "—ç—ç" intact

    # **5. Ensure nominative ("NOM") forms use "—ç—Ä"**
    if "NOM" in msd and not base.endswith("—ç—Ä"):
        base = base[:-1] + "—ç—Ä" if base[-1] == "—Ä" else base + "—ç—Ä"

    return base

def apply_best_rule(lemma, msd, allprules, allsrules):
    """Applies the longest-matching suffix-changing rule given an input
    form and the MSD. Length ties in suffix rules are broken by frequency.
    For prefix-changing rules, only the most frequent rule is chosen."""

    base = "<" + lemma + ">"
    if msd not in allprules and msd not in allsrules:
        return lemma  # Haven't seen this inflection, so return lemma unchanged

    # Apply suffix rules first
    if msd in allsrules:
        applicablerules = [(x[0], x[1], y) for x, y in allsrules[msd].items() if x[0] in base]
        if applicablerules:
            bestrule = max(applicablerules, key=lambda x: (len(x[0]), x[2], len(x[1])))
            base = base.replace(bestrule[0], bestrule[1])

    # Apply prefix rules (if applicable)
    if msd in allprules:
        applicablerules = [(x[0], x[1], y) for x, y in allprules[msd].items() if x[0] in base]
        if applicablerules:
            bestrule = max(applicablerules, key=lambda x: x[2])
            base = base.replace(bestrule[0], bestrule[1])

    base = base.replace("<", "").replace(">", "")

    # **üìå Ï∂îÍ∞ÄÎêú Kabardian Ïò§Î•ò ÏàòÏ†ï Í∑úÏπô**

    # 1Ô∏è‚É£ ERG/INSÏóêÏÑú "—ç—Ö—ç" Í≥ºÏûâ ÏÇ¨Ïö© Î∞©ÏßÄ ‚Üí "—Ä" ÎòêÎäî "—ã—É" Î≥ÄÍ≤Ω
    if msd in ["ERG;SG;DEF", "INS;SG;DEF"] and base.endswith("—ç—Ö—ç"):
        base = base[:-2] + "—Ä"  # "—ç—Ö—ç" ‚Üí "—Ä"

    # 2Ô∏è‚É£ Î≥µÏàòÌòï Ïò§Î•ò ÏàòÏ†ï (PL) ‚Üí "—ç—Ö—ç"Î•º "—ã—É"Î°ú Î≥ÄÍ≤Ω
    if "PL" in msd and base.endswith("—ç—Ö—ç"):
        base = base[:-2] + "—ã—É"  # "—ç—Ö—ç" ‚Üí "—ã—É"

    # 3Ô∏è‚É£ NOM;SGÏóêÏÑú Î∂àÌïÑÏöîÌïú "—ç—Ä" Ï†úÍ±∞
    if msd == "NOM;SG" and base.endswith("—ç—Ä"):
        base = base[:-2]  # "—ç—Ä" Ï†úÍ±∞

    # 4Ô∏è‚É£ INS Î≥ÄÌòï Ïãú Ïò§Î•ò Î∞©ÏßÄ (SG/PL)
    if "INS" in msd and base.endswith("—ç—Ö—ç"):
        base = base[:-2] + "—ã—É"  # "—ç—Ö—ç" ‚Üí "—ã—É"
    elif "INS" in msd and not base.endswith("—ã—É"):
        base += "—É"  # ÎèÑÍµ¨Í≤© Îã®ÏàòÌòï Î≥¥Ï†ï

    # 5Ô∏è‚É£ "-–∫”è—ç" Í≥ºÏûâ ÏÇ¨Ïö© Î∞©ÏßÄ (ERG/INS Î≥ÄÌòï Ïãú)
    if base.endswith("–∫”è—ç") and msd in ["ERG;SG;DEF", "INS;SG;DEF"]:
        base = base[:-3] + "—Ä"  # "–∫”è—ç" ‚Üí "—Ä"

    return base

import sys
from collections import defaultdict

lang = "swc"

# Rule storage
allprules, allsrules = defaultdict(lambda: defaultdict(int)), defaultdict(lambda: defaultdict(int))
trainlemmas, trainmsds = set(), set()

# Load training data
train_file = f"/content/{lang}.train.tsv"
lines = [line.strip() for line in open(train_file, "r") if line != "\n"]

# Detect if language is predominantly prefixing or suffixing
prefbias, suffbias = 0, 0
lemma_groups = defaultdict(list)

for l in lines:
    lemma, form, msd = l.split("\t")
    lemma_groups[lemma].append((form, msd))
    trainlemmas.add(lemma)
    trainmsds.add(msd)

    aligned = halign(lemma, form)
    if " " not in aligned[0] and " " not in aligned[1] and "-" not in aligned[0] and "-" not in aligned[1]:
        prefbias += numleadingsyms(aligned[0], "_") + numleadingsyms(aligned[1], "_")
        suffbias += numtrailingsyms(aligned[0], "_") + numtrailingsyms(aligned[1], "_")

# Process rules per lemma group
for lemma, forms in lemma_groups.items():
    if prefbias > suffbias:
        lemma = lemma[::-1]  # Reverse for prefixing languages

    for form, msd in forms:
        transformed_lemma = lemma[::-1] if prefbias > suffbias else lemma
        transformed_form = form[::-1] if prefbias > suffbias else form

        # Extract rules
        prules, srules = prefix_suffix_rules_get(transformed_lemma, transformed_form)

        # Store rules with frequency weighting
        for rule in prules:
            allprules[msd][rule] += 1
        for rule in srules:
            allsrules[msd][rule] += 1

# Load evaluation dataset
eval_file = f"/content/{lang}.dev.tsv"
evallines = [line.strip() for line in open(eval_file, "r") if line != "\n"]

# Output file
outfile = open(f"{lang}.txt", "w")

pred, gold = [], []

# Apply learned rules to the dev set
for l in evallines:
    lemma, correct, msd = l.split("\t")
    transformed_lemma = lemma[::-1] if prefbias > suffbias else lemma

    # Apply the best-matching rule
    outform = apply_best_rule(transformed_lemma, msd, allprules, allsrules)

    # Reverse back if necessary
    if prefbias > suffbias:
        outform = outform[::-1]

    pred.append(outform)
    gold.append(correct)
    outfile.write(outform + "\n")

outfile.close()

# Evaluate Performance
print("Predictions:", pred[:10])
print("Gold Standard:", gold[:10])
print("Evaluation Score:", evaluate(pred, gold))

# Load test dataset (ROOT INFLECTION format)
test_file = f"/content/{lang}.test.tsv"
test_lines = [line.strip() for line in open(test_file, "r") if line != "\n"]

# Output file for predictions
output_file = f"/content/{lang}.txt"
outfile = open(output_file, "w")

# Predict inflected forms
predictions = []

for l in test_lines:
    lemma, msd = l.split("\t")  # Test format: [ROOT] [INFLECTION]

    if msd not in allprules and msd not in allsrules:
      print(f"Warning: Unseen inflection type {msd} for lemma {lemma}")

    transformed_lemma = lemma[::-1] if prefbias > suffbias else lemma

    # Apply best rule
    predicted_form = apply_best_rule(transformed_lemma, msd, allprules, allsrules)

    # Reverse back if necessary
    if prefbias > suffbias:
        predicted_form = predicted_form[::-1]

    predictions.append(predicted_form)
    outfile.write(predicted_form + "\n")

outfile.close()

print(f"Predictions saved in: {output_file}")
print(predictions[:10])  # Print sample predictions

def compare_predictions(pred, gold):
    """Compares predictions and gold standard, displaying incorrect cases."""

    incorrect_cases = []

    for i, (p, g) in enumerate(zip(pred, gold)):
        if p != g:
            incorrect_cases.append((i, p, g))

    # Print incorrect cases
    print(f"Total incorrect cases: {len(incorrect_cases)} / {len(gold)}")

    for i, p, g in incorrect_cases:
        print(f"[{i}] ‚ùå PRED: {p} | ‚úÖ GOLD: {g}")

# Example usage (Replace pred and gold with actual lists)
compare_predictions(pred, gold)

"""original


> Add blockquote


```
# This is formatted as code
```


"""

lang = "kbd"

allprules, allsrules = {}, {}
lines = [line.strip() for line in open(f"/content/{lang}.train.tsv", "r") if line != '\n']
trainlemmas = set()
trainmsds = set()

# First, test if language is predominantly suffixing or prefixing
# If prefixing, work with reversed strings
prefbias, suffbias = 0,0
for l in lines:
  lemma, form, msd = l.split(u'\t')
  trainlemmas.add(lemma)
  trainmsds.add(msd)
  aligned = halign(lemma, form)
  if ' ' not in aligned[0] and ' ' not in aligned[1] and '-' not in aligned[0] and '-' not in aligned[1]:
      prefbias += numleadingsyms(aligned[0],'_') + numleadingsyms(aligned[1],'_')
      suffbias += numtrailingsyms(aligned[0],'_') + numtrailingsyms(aligned[1],'_')
for l in lines: # Read in lines and extract transformation rules from pairs
    lemma, form, msd = l.split(u'\t')
    if prefbias > suffbias:
        lemma = lemma[::-1]
        form = form[::-1]
    prules, srules = prefix_suffix_rules_get(lemma, form)

    if msd not in allprules and len(prules) > 0:
        allprules[msd] = {}
    if msd not in allsrules and len(srules) > 0:
        allsrules[msd] = {}

    for r in prules:
        if (r[0],r[1]) in allprules[msd]:
            allprules[msd][(r[0],r[1])] = allprules[msd][(r[0],r[1])] + 1
        else:
            allprules[msd][(r[0],r[1])] = 1

    for r in srules:
        if (r[0],r[1]) in allsrules[msd]:
            allsrules[msd][(r[0],r[1])] = allsrules[msd][(r[0],r[1])] + 1
        else:
            allsrules[msd][(r[0],r[1])] = 1

evallines = [line.strip() for line in open(f"/content/{lang}.dev.tsv", "r") if line != '\n']
outfile = open(f"{lang}.txt", "w")

pred = []
gold = []

for l in evallines:
    lemma, correct, msd = l.split(u'\t')
    if prefbias > suffbias:
        lemma = lemma[::-1]
    outform = apply_best_rule(lemma, msd, allprules, allsrules)
    if prefbias > suffbias:
        outform = outform[::-1]
        lemma = lemma[::-1]
    pred.append(outform)
    gold.append(correct)

    outfile.write(outform + "\n")

print(pred[:10])
print(gold[:10])
print(evaluate(pred, gold))